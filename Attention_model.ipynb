{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CidlQR0p3dZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from builtins import range, input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UIYdSPJ3nZS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "5ae909ab-927e-49e9-b122-ccb9fe500b60"
      },
      "source": [
        "import os, sys\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, GRU, Dense, Embedding, \\\n",
        "  Bidirectional, RepeatVector, Concatenate, Activation, Dot, Lambda\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras.backend as K\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if len(K.tensorflow_backend._get_available_gpus()) > 0:\n",
        "  from keras.layers import CuDNNLSTM as LSTM\n",
        "  from keras.layers import CuDNNGRU as GRU\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_AI_v013uy6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "3e99d974-217a-4c64-e846-9bcead0e6a1f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED_brxR433Cu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make sure we do softmax over the time axis\n",
        "# expected shape is N x T x D\n",
        "def softmax_over_time(x):\n",
        "  assert(K.ndim(x) > 2)\n",
        "  e = K.exp(x - K.max(x, axis=1, keepdims=True))\n",
        "  s = K.sum(e, axis=1, keepdims=True)\n",
        "  return e / s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98VBJtA84Otp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# config\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "LATENT_DIM = 256\n",
        "LATENT_DIM_DECODER = 256 # idea: make it different to ensure things all fit together properly!\n",
        "NUM_SAMPLES = 10000\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hotOW1dv4VVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Where we will store the data\n",
        "input_texts = [] # sentence in original language\n",
        "target_texts = [] # sentence in target language\n",
        "target_texts_inputs = [] # sentence in target language offset by 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3IDvVmY4YfC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70cfd05e-ebf1-49d8-ff58-0725fb2cf528"
      },
      "source": [
        "# load in the data\n",
        "t = 0\n",
        "for line in open('/content/drive/My Drive/Language Translation/fra.txt'):\n",
        "  # only keep a limited number of samples\n",
        "  t += 1\n",
        "  if t > NUM_SAMPLES:\n",
        "    break\n",
        "\n",
        "  # input and target are separated by tab\n",
        "  if '\\t' not in line:\n",
        "    continue\n",
        "\n",
        "  # split up the input and translation\n",
        "  input_text, translation = line.rstrip().split('\\t')\n",
        "\n",
        "  # make the target input and output\n",
        "  # recall we'll be using teacher forcing\n",
        "  target_text = translation + ' <eos>'\n",
        "  target_text_input = '<sos> ' + translation\n",
        "\n",
        "  input_texts.append(input_text)\n",
        "  target_texts.append(target_text)\n",
        "  target_texts_inputs.append(target_text_input)\n",
        "print(\"num samples:\", len(input_texts))\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num samples: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmUWDaF44i4b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb285d50-5e28-4b17-f794-1d92603cf178"
      },
      "source": [
        "# tokenize the inputs\n",
        "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer_inputs.fit_on_texts(input_texts)\n",
        "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
        "\n",
        "# get the word to index mapping for input language\n",
        "word2idx_inputs = tokenizer_inputs.word_index\n",
        "print('Found %s unique input tokens.' % len(word2idx_inputs))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2139 unique input tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA562iRF4nMZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a263532-b7d0-47a2-c8dd-9fa33d04c9a3"
      },
      "source": [
        "# determine maximum length input sequence\n",
        "max_len_input = max(len(s) for s in input_sequences)\n",
        "\n",
        "# tokenize the outputs\n",
        "# don't filter out special characters\n",
        "# otherwise <sos> and <eos> won't appear\n",
        "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
        "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) # inefficient, oh well\n",
        "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
        "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)\n",
        "\n",
        "# get the word to index mapping for output language\n",
        "word2idx_outputs = tokenizer_outputs.word_index\n",
        "print('Found %s unique output tokens.' % len(word2idx_outputs))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5771 unique output tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMJifsn84trC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# store number of output words for later\n",
        "# remember to add 1 since indexing starts at 1\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "\n",
        "# determine maximum length output sequence\n",
        "max_len_target = max(len(s) for s in target_sequences)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7Vb-tek4yqV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "42da4fd8-cb5c-4237-8b83-02f2575d1da4"
      },
      "source": [
        "# pad the sequences\n",
        "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
        "print(\"encoder_data.shape:\", encoder_inputs.shape)\n",
        "print(\"encoder_data[0]:\", encoder_inputs[0])\n",
        "\n",
        "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n",
        "print(\"decoder_data[0]:\", decoder_inputs[0])\n",
        "print(\"decoder_data.shape:\", decoder_inputs.shape)\n",
        "\n",
        "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder_data.shape: (10000, 5)\n",
            "encoder_data[0]: [ 0  0  0  0 14]\n",
            "decoder_data[0]: [ 2 56  4  0  0  0  0  0  0  0  0]\n",
            "decoder_data.shape: (10000, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb-LzHhi43TL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "9ad01043-d330-4d0e-d56d-4b46f91b01fb"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-30 13:38:31--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-10-30 13:38:32--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2019-10-30 13:38:32--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.88MB/s    in 6m 27s  \n",
            "\n",
            "2019-10-30 13:44:59 (2.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgdjLUhX6f8H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8673d2c2-bcaf-432a-981a-f72682ae1772"
      },
      "source": [
        "!unzip glove*.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12FPD8BA6nnE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fcb565af-dd3b-473b-83d8-f4263e681870"
      },
      "source": [
        "# store all the pre-trained word vectors\n",
        "import io\n",
        "print('Loading word vectors...')\n",
        "word2vec = {}\n",
        "with io.open('glove.6B.50d.txt', encoding='utf8') as f:\n",
        "  # is just a space-separated text file in the format:\n",
        "  # word vec[0] vec[1] vec[2] ...\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vec = np.asarray(values[1:], dtype='float32')\n",
        "    word2vec[word] = vec\n",
        "print('Found %s word vectors.' % len(word2vec))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading word vectors...\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fms9-Hdn6xGP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3df4baec-546b-4bf9-80f1-ed86296abf30"
      },
      "source": [
        "# prepare embedding matrix\n",
        "print('Filling pre-trained embeddings...')\n",
        "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word2idx_inputs.items():\n",
        "  if i < MAX_NUM_WORDS:\n",
        "    embedding_vector = word2vec.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all zeros.\n",
        "      embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filling pre-trained embeddings...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJNZU8kr6y_U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "049c5162-8538-4a72-c74f-90877055fbc5"
      },
      "source": [
        "# create embedding layer\n",
        "embedding_layer = Embedding(\n",
        "  num_words,\n",
        "  EMBEDDING_DIM,\n",
        "  weights=[embedding_matrix],\n",
        "  input_length=max_len_input,\n",
        "  # trainable=True\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-AHYxdG63ES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create targets, since we cannot use sparse\n",
        "# categorical cross entropy when we have sequences\n",
        "decoder_targets_one_hot = np.zeros(\n",
        "  (\n",
        "    len(input_texts),\n",
        "    max_len_target,\n",
        "    num_words_output\n",
        "  ),\n",
        "  dtype='float32'\n",
        ")\n",
        "\n",
        "# assign the values\n",
        "for i, d in enumerate(decoder_targets):\n",
        "  for t, word in enumerate(d):\n",
        "    decoder_targets_one_hot[i, t, word] = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFBfel6Y7IB_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ed8c463c-3d2e-43c6-94df-a29732aa2c8d"
      },
      "source": [
        "##### build the model #####\n",
        "\n",
        "# Set up the encoder - simple!\n",
        "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
        "x = embedding_layer(encoder_inputs_placeholder)\n",
        "encoder = Bidirectional(LSTM(\n",
        "  LATENT_DIM,\n",
        "  return_sequences=True,\n",
        "  # dropout=0.5 # dropout not available on gpu\n",
        "))\n",
        "encoder_outputs = encoder(x)\n",
        "\n",
        "\n",
        "# Set up the decoder - not so simple\n",
        "decoder_inputs_placeholder = Input(shape=(max_len_target,))\n",
        "\n",
        "# this word embedding will not use pre-trained vectors\n",
        "# although you could\n",
        "decoder_embedding = Embedding(num_words_output, EMBEDDING_DIM)\n",
        "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######### Attention #########\n",
        "# Attention layers need to be global because\n",
        "# they will be repeated Ty times at the decoder\n",
        "attn_repeat_layer = RepeatVector(max_len_input)\n",
        "attn_concat_layer = Concatenate(axis=-1)\n",
        "attn_dense1 = Dense(10, activation='tanh')\n",
        "attn_dense2 = Dense(1, activation=softmax_over_time)\n",
        "attn_dot = Dot(axes=1) # to perform the weighted sum of alpha[t] * h[t]\n",
        "\n",
        "def one_step_attention(h, st_1):\n",
        "  # h = h(1), ..., h(Tx), shape = (Tx, LATENT_DIM * 2)\n",
        "  # st_1 = s(t-1), shape = (LATENT_DIM_DECODER,)\n",
        " \n",
        "  # copy s(t-1) Tx times\n",
        "  # now shape = (Tx, LATENT_DIM_DECODER)\n",
        "  st_1 = attn_repeat_layer(st_1)\n",
        "\n",
        "  # Concatenate all h(t)'s with s(t-1)\n",
        "  # Now of shape (Tx, LATENT_DIM_DECODER + LATENT_DIM * 2)\n",
        "  x = attn_concat_layer([h, st_1])\n",
        "\n",
        "  # Neural net first layer\n",
        "  x = attn_dense1(x)\n",
        "\n",
        "  # Neural net second layer with special softmax over time\n",
        "  alphas = attn_dense2(x)\n",
        "\n",
        "  # \"Dot\" the alphas and the h's\n",
        "  # Remember a.dot(b) = sum over a[t] * b[t]\n",
        "  context = attn_dot([alphas, h])\n",
        "\n",
        "  return context\n",
        "\n",
        "\n",
        "# define the rest of the decoder (after attention)\n",
        "decoder_lstm = LSTM(LATENT_DIM_DECODER, return_state=True)\n",
        "decoder_dense = Dense(num_words_output, activation='softmax')\n",
        "\n",
        "initial_s = Input(shape=(LATENT_DIM_DECODER,), name='s0')\n",
        "initial_c = Input(shape=(LATENT_DIM_DECODER,), name='c0')\n",
        "context_last_word_concat_layer = Concatenate(axis=2)\n",
        "\n",
        "\n",
        "# Unlike previous seq2seq, we cannot get the output\n",
        "# all in one step\n",
        "# Instead we need to do Ty steps\n",
        "# And in each of those steps, we need to consider\n",
        "# all Tx h's\n",
        "\n",
        "# s, c will be re-assigned in each iteration of the loop\n",
        "s = initial_s\n",
        "c = initial_c\n",
        "\n",
        "# collect outputs in a list at first\n",
        "outputs = []\n",
        "for t in range(max_len_target): # Ty times\n",
        "  # get the context using attention\n",
        "  context = one_step_attention(encoder_outputs, s)\n",
        "\n",
        "  # we need a different layer for each time step\n",
        "  selector = Lambda(lambda x: x[:, t:t+1])\n",
        "  xt = selector(decoder_inputs_x)\n",
        "  \n",
        "  # combine \n",
        "  decoder_lstm_input = context_last_word_concat_layer([context, xt])\n",
        "\n",
        "  # pass the combined [context, last word] into the LSTM\n",
        "  # along with [s, c]\n",
        "  # get the new [s, c] and output\n",
        "  o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s, c])\n",
        "\n",
        "  # final dense layer to get next word prediction\n",
        "  decoder_outputs = decoder_dense(o)\n",
        "  outputs.append(decoder_outputs)\n",
        "\n",
        "\n",
        "# 'outputs' is now a list of length Ty\n",
        "# each element is of shape (batch size, output vocab size)\n",
        "# therefore if we simply stack all the outputs into 1 tensor\n",
        "# it would be of shape T x N x D\n",
        "# we would like it to be of shape N x T x D\n",
        "\n",
        "def stack_and_transpose(x):\n",
        "  # x is a list of length T, each element is a batch_size x output_vocab_size tensor\n",
        "  x = K.stack(x) # is now T x batch_size x output_vocab_size tensor\n",
        "  x = K.permute_dimensions(x, pattern=(1, 0, 2)) # is now batch_size x T x output_vocab_size\n",
        "  return x\n",
        "\n",
        "# make it a layer\n",
        "stacker = Lambda(stack_and_transpose)\n",
        "outputs = stacker(outputs)\n",
        "\n",
        "# create the model\n",
        "model = Model(\n",
        "  inputs=[\n",
        "    encoder_inputs_placeholder,\n",
        "    decoder_inputs_placeholder,\n",
        "    initial_s, \n",
        "    initial_c,\n",
        "  ],\n",
        "  outputs=outputs\n",
        ")\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "z = np.zeros((NUM_SAMPLES, LATENT_DIM_DECODER)) # initial [s, c]\n",
        "r = model.fit(\n",
        "  [encoder_inputs, decoder_inputs, z, z], decoder_targets_one_hot,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  epochs=EPOCHS,\n",
        "  validation_split=0.2\n",
        ")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 19s 2ms/step - loss: 2.5066 - acc: 0.6444 - val_loss: 2.3736 - val_acc: 0.6173\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 1.8592 - acc: 0.7200 - val_loss: 2.0971 - val_acc: 0.6984\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 1.6391 - acc: 0.7418 - val_loss: 1.9672 - val_acc: 0.7173\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 1.4810 - acc: 0.7649 - val_loss: 1.8506 - val_acc: 0.7377\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 1.3531 - acc: 0.7840 - val_loss: 1.7864 - val_acc: 0.7435\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 1.2507 - acc: 0.7989 - val_loss: 1.7230 - val_acc: 0.7620\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 1.1647 - acc: 0.8080 - val_loss: 1.6757 - val_acc: 0.7672\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 1.0871 - acc: 0.8162 - val_loss: 1.6764 - val_acc: 0.7711\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 1.0169 - acc: 0.8244 - val_loss: 1.6446 - val_acc: 0.7744\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.9571 - acc: 0.8316 - val_loss: 1.6158 - val_acc: 0.7789\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.9071 - acc: 0.8387 - val_loss: 1.6082 - val_acc: 0.7803\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.8589 - acc: 0.8449 - val_loss: 1.6066 - val_acc: 0.7823\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.8163 - acc: 0.8506 - val_loss: 1.5995 - val_acc: 0.7831\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.7763 - acc: 0.8572 - val_loss: 1.6071 - val_acc: 0.7827\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.7381 - acc: 0.8630 - val_loss: 1.6030 - val_acc: 0.7836\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.6994 - acc: 0.8694 - val_loss: 1.5841 - val_acc: 0.7856\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.6624 - acc: 0.8742 - val_loss: 1.6000 - val_acc: 0.7849\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.6275 - acc: 0.8799 - val_loss: 1.5981 - val_acc: 0.7856\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.5972 - acc: 0.8846 - val_loss: 1.6037 - val_acc: 0.7855\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.5678 - acc: 0.8905 - val_loss: 1.6096 - val_acc: 0.7889\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.5405 - acc: 0.8946 - val_loss: 1.6321 - val_acc: 0.7864\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.5142 - acc: 0.8983 - val_loss: 1.6258 - val_acc: 0.7854\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.4925 - acc: 0.9020 - val_loss: 1.6329 - val_acc: 0.7844\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4744 - acc: 0.9049 - val_loss: 1.6391 - val_acc: 0.7852\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4550 - acc: 0.9075 - val_loss: 1.6405 - val_acc: 0.7850\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4357 - acc: 0.9104 - val_loss: 1.6473 - val_acc: 0.7850\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4185 - acc: 0.9129 - val_loss: 1.6618 - val_acc: 0.7830\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.4045 - acc: 0.9149 - val_loss: 1.6812 - val_acc: 0.7827\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3901 - acc: 0.9169 - val_loss: 1.6798 - val_acc: 0.7800\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3772 - acc: 0.9185 - val_loss: 1.6677 - val_acc: 0.7845\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3630 - acc: 0.9208 - val_loss: 1.6954 - val_acc: 0.7818\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3486 - acc: 0.9226 - val_loss: 1.6803 - val_acc: 0.7815\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3359 - acc: 0.9241 - val_loss: 1.6873 - val_acc: 0.7804\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3256 - acc: 0.9257 - val_loss: 1.6977 - val_acc: 0.7825\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3174 - acc: 0.9276 - val_loss: 1.7254 - val_acc: 0.7785\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.3066 - acc: 0.9284 - val_loss: 1.7321 - val_acc: 0.7790\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2952 - acc: 0.9295 - val_loss: 1.7221 - val_acc: 0.7800\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2861 - acc: 0.9311 - val_loss: 1.7219 - val_acc: 0.7799\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2801 - acc: 0.9318 - val_loss: 1.7557 - val_acc: 0.7761\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2746 - acc: 0.9328 - val_loss: 1.7561 - val_acc: 0.7780\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2694 - acc: 0.9330 - val_loss: 1.7621 - val_acc: 0.7766\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2645 - acc: 0.9339 - val_loss: 1.7673 - val_acc: 0.7770\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2586 - acc: 0.9358 - val_loss: 1.7707 - val_acc: 0.7760\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2541 - acc: 0.9358 - val_loss: 1.7765 - val_acc: 0.7778\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2495 - acc: 0.9367 - val_loss: 1.7709 - val_acc: 0.7770\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2445 - acc: 0.9371 - val_loss: 1.7914 - val_acc: 0.7755\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2402 - acc: 0.9380 - val_loss: 1.7935 - val_acc: 0.7756\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2358 - acc: 0.9376 - val_loss: 1.8081 - val_acc: 0.7755\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2321 - acc: 0.9386 - val_loss: 1.8245 - val_acc: 0.7757\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2282 - acc: 0.9393 - val_loss: 1.8215 - val_acc: 0.7740\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2240 - acc: 0.9400 - val_loss: 1.8380 - val_acc: 0.7747\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2203 - acc: 0.9400 - val_loss: 1.8365 - val_acc: 0.7740\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2159 - acc: 0.9414 - val_loss: 1.8335 - val_acc: 0.7740\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2125 - acc: 0.9411 - val_loss: 1.8387 - val_acc: 0.7740\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2084 - acc: 0.9412 - val_loss: 1.8606 - val_acc: 0.7733\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2048 - acc: 0.9419 - val_loss: 1.8595 - val_acc: 0.7728\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.2009 - acc: 0.9426 - val_loss: 1.8654 - val_acc: 0.7734\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1974 - acc: 0.9425 - val_loss: 1.8822 - val_acc: 0.7713\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1929 - acc: 0.9431 - val_loss: 1.8674 - val_acc: 0.7734\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1888 - acc: 0.9433 - val_loss: 1.8665 - val_acc: 0.7730\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1857 - acc: 0.9432 - val_loss: 1.8806 - val_acc: 0.7735\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1828 - acc: 0.9436 - val_loss: 1.8882 - val_acc: 0.7720\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1800 - acc: 0.9433 - val_loss: 1.8832 - val_acc: 0.7733\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1775 - acc: 0.9442 - val_loss: 1.8975 - val_acc: 0.7717\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1746 - acc: 0.9440 - val_loss: 1.8991 - val_acc: 0.7736\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1725 - acc: 0.9441 - val_loss: 1.9138 - val_acc: 0.7717\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1698 - acc: 0.9447 - val_loss: 1.9025 - val_acc: 0.7719\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1682 - acc: 0.9447 - val_loss: 1.9069 - val_acc: 0.7714\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1669 - acc: 0.9444 - val_loss: 1.9073 - val_acc: 0.7711\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1646 - acc: 0.9440 - val_loss: 1.9102 - val_acc: 0.7720\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1629 - acc: 0.9447 - val_loss: 1.9287 - val_acc: 0.7715\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1601 - acc: 0.9451 - val_loss: 1.9255 - val_acc: 0.7713\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1584 - acc: 0.9449 - val_loss: 1.9260 - val_acc: 0.7716\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1572 - acc: 0.9450 - val_loss: 1.9516 - val_acc: 0.7697\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1555 - acc: 0.9454 - val_loss: 1.9476 - val_acc: 0.7709\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1540 - acc: 0.9453 - val_loss: 1.9431 - val_acc: 0.7709\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1523 - acc: 0.9457 - val_loss: 1.9574 - val_acc: 0.7698\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1511 - acc: 0.9456 - val_loss: 1.9633 - val_acc: 0.7701\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1496 - acc: 0.9451 - val_loss: 1.9636 - val_acc: 0.7713\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1483 - acc: 0.9455 - val_loss: 1.9779 - val_acc: 0.7705\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1465 - acc: 0.9459 - val_loss: 1.9767 - val_acc: 0.7704\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1456 - acc: 0.9447 - val_loss: 1.9793 - val_acc: 0.7703\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1438 - acc: 0.9460 - val_loss: 1.9831 - val_acc: 0.7710\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1430 - acc: 0.9458 - val_loss: 1.9995 - val_acc: 0.7700\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1418 - acc: 0.9459 - val_loss: 1.9934 - val_acc: 0.7710\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1406 - acc: 0.9457 - val_loss: 2.0032 - val_acc: 0.7708\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1395 - acc: 0.9458 - val_loss: 2.0112 - val_acc: 0.7697\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1389 - acc: 0.9452 - val_loss: 2.0126 - val_acc: 0.7700\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1376 - acc: 0.9456 - val_loss: 2.0241 - val_acc: 0.7698\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1369 - acc: 0.9453 - val_loss: 2.0161 - val_acc: 0.7701\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1355 - acc: 0.9463 - val_loss: 2.0182 - val_acc: 0.7694\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1348 - acc: 0.9453 - val_loss: 2.0314 - val_acc: 0.7701\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1345 - acc: 0.9454 - val_loss: 2.0465 - val_acc: 0.7692\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1333 - acc: 0.9455 - val_loss: 2.0346 - val_acc: 0.7706\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1329 - acc: 0.9456 - val_loss: 2.0396 - val_acc: 0.7696\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1320 - acc: 0.9453 - val_loss: 2.0407 - val_acc: 0.7710\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1310 - acc: 0.9452 - val_loss: 2.0364 - val_acc: 0.7687\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1309 - acc: 0.9449 - val_loss: 2.0479 - val_acc: 0.7699\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 13s 2ms/step - loss: 0.1296 - acc: 0.9453 - val_loss: 2.0518 - val_acc: 0.7702\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 14s 2ms/step - loss: 0.1289 - acc: 0.9452 - val_loss: 2.0623 - val_acc: 0.7700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88U7asW3FnqW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "c3dd2d3c-dbdb-4910-f2c5-9e1070577d2b"
      },
      "source": [
        "# plot some data\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8HU1DtwGBB0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "62f124da-1d0f-4968-c100-c2c3194ca2ad"
      },
      "source": [
        "# accuracies\n",
        "plt.plot(r.history['acc'], label='acc')\n",
        "plt.plot(r.history['val_acc'], label='val_acc')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esVgxcR_GMpg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Make predictions #####\n",
        "# As with the poetry example, we need to create another model\n",
        "# that can take in the RNN state and previous word as input\n",
        "# and accept a T=1 sequence.\n",
        "\n",
        "# The encoder will be stand-alone\n",
        "# From this we will get our initial decoder hidden state\n",
        "# i.e. h(1), ..., h(Tx)\n",
        "encoder_model = Model(encoder_inputs_placeholder, encoder_outputs)\n",
        "\n",
        "# next we define a T=1 decoder model\n",
        "encoder_outputs_as_input = Input(shape=(max_len_input, LATENT_DIM * 2,))\n",
        "decoder_inputs_single = Input(shape=(1,))\n",
        "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
        "\n",
        "# no need to loop over attention steps this time because there is only one step\n",
        "context = one_step_attention(encoder_outputs_as_input, initial_s)\n",
        "\n",
        "# combine context with last word\n",
        "decoder_lstm_input = context_last_word_concat_layer([context, decoder_inputs_single_x])\n",
        "\n",
        "# lstm and final dense\n",
        "o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\n",
        "decoder_outputs = decoder_dense(o)\n",
        "\n",
        "\n",
        "# note: we don't really need the final stack and tranpose\n",
        "# because there's only 1 output\n",
        "# it is already of size N x D\n",
        "# no need to make it 1 x N x D --> N x 1 x D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnqUkbkHGVqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the model object\n",
        "decoder_model = Model(\n",
        "  inputs=[\n",
        "    decoder_inputs_single,\n",
        "    encoder_outputs_as_input,\n",
        "    initial_s, \n",
        "    initial_c\n",
        "  ],\n",
        "  outputs=[decoder_outputs, s, c]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# map indexes back into real words\n",
        "# so we can view the results\n",
        "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
        "idx2word_trans = {v:k for k, v in word2idx_outputs.items()}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2VCXnBmGaaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "  # Encode the input as state vectors.\n",
        "  enc_out = encoder_model.predict(input_seq)\n",
        "\n",
        "  # Generate empty target sequence of length 1.\n",
        "  target_seq = np.zeros((1, 1))\n",
        "  \n",
        "  # Populate the first character of target sequence with the start character.\n",
        "  # NOTE: tokenizer lower-cases all words\n",
        "  target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "\n",
        "  # if we get this we break\n",
        "  eos = word2idx_outputs['<eos>']\n",
        "\n",
        "\n",
        "  # [s, c] will be updated in each loop iteration\n",
        "  s = np.zeros((1, LATENT_DIM_DECODER))\n",
        "  c = np.zeros((1, LATENT_DIM_DECODER))\n",
        "\n",
        "\n",
        "  # Create the translation\n",
        "  output_sentence = []\n",
        "  for _ in range(max_len_target):\n",
        "    o, s, c = decoder_model.predict([target_seq, enc_out, s, c])\n",
        "        \n",
        "\n",
        "    # Get next word\n",
        "    idx = np.argmax(o.flatten())\n",
        "\n",
        "    # End sentence of EOS\n",
        "    if eos == idx:\n",
        "      break\n",
        "\n",
        "    word = ''\n",
        "    if idx > 0:\n",
        "      word = idx2word_trans[idx]\n",
        "      output_sentence.append(word)\n",
        "\n",
        "    # Update the decoder input\n",
        "    # which is just the word just generated\n",
        "    target_seq[0, 0] = idx\n",
        "\n",
        "  return ' '.join(output_sentence)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GN2e0QtGfEz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "f5e794d6-63c9-408a-b55a-9025134adac9"
      },
      "source": [
        "while True:\n",
        "  # Do some test translations\n",
        "  i = np.random.choice(len(input_texts))\n",
        "  input_seq = encoder_inputs[i:i+1]\n",
        "  translation = decode_sequence(input_seq)\n",
        "  print('-')\n",
        "  print('Input sentence:', input_texts[i])\n",
        "  print('Predicted translation:', translation)\n",
        "  print('Actual translation:', target_texts[i])\n",
        "\n",
        "  ans = input(\"Continue? [Y/n]\")\n",
        "  if ans and ans.lower().startswith('n'):\n",
        "    break\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: Be discreet.\n",
            "Predicted translation: soyez discrète !\n",
            "Actual translation: Sois discrète ! <eos>\n",
            "Continue? [Y/n]Y\n",
            "-\n",
            "Input sentence: Let me talk.\n",
            "Predicted translation: laissez-moi parler.\n",
            "Actual translation: Laissez-moi parler. <eos>\n",
            "Continue? [Y/n]Y\n",
            "-\n",
            "Input sentence: Tom forgot.\n",
            "Predicted translation: tom oublia.\n",
            "Actual translation: Tom oublia. <eos>\n",
            "Continue? [Y/n]Y\n",
            "-\n",
            "Input sentence: They loved Tom.\n",
            "Predicted translation: elles aiment tom.\n",
            "Actual translation: Ils aimaient Tom. <eos>\n",
            "Continue? [Y/n]Y\n",
            "-\n",
            "Input sentence: You're funny.\n",
            "Predicted translation: vous êtes marrante.\n",
            "Actual translation: Vous êtes marrant. <eos>\n",
            "Continue? [Y/n]Y\n",
            "-\n",
            "Input sentence: Are you relaxed?\n",
            "Predicted translation: êtes-vous content ?\n",
            "Actual translation: Êtes-vous détendues ? <eos>\n",
            "Continue? [Y/n]n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRWlA8B0GYQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}