{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jJ5_yNZouk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from builtins import range, input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhRaGgojo5ln",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "31c071c8-f2a6-4415-abc1-95928ed2b03b"
      },
      "source": [
        "import os, sys\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras.backend as K\n",
        "if len(K.tensorflow_backend._get_available_gpus()) > 0:\n",
        "  from keras.layers import CuDNNLSTM as LSTM\n",
        "  from keras.layers import CuDNNGRU as GRU"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo0OxZFxo_kB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "be36f0ee-2e02-41ae-c0f7-bc75a25b0c14"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JTxKFiQpc_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# some config\n",
        "BATCH_SIZE = 64  # Batch size for training.\n",
        "EPOCHS = 100  # Number of epochs to train for.\n",
        "LATENT_DIM = 256  # Latent dimensionality of the encoding space.\n",
        "NUM_SAMPLES = 10000  # Number of samples to train on.\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 50\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCJDexNjpkzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Where we will store the data\n",
        "input_texts = [] # sentence in original language\n",
        "target_texts = [] # sentence in target language\n",
        "target_texts_inputs = [] # sentence in target language offset by 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5CrOjqcpoz9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e31f4c6f-05c0-45ae-d7c3-29a931ec9859"
      },
      "source": [
        "# load in the data\n",
        "t = 0\n",
        "for line in open('/content/drive/My Drive/Language Translation/fra.txt'):\n",
        "  # only keep a limited number of samples\n",
        "  t += 1\n",
        "  if t > NUM_SAMPLES:\n",
        "    break\n",
        "\n",
        "  # input and target are separated by tab\n",
        "  if '\\t' not in line:\n",
        "    continue\n",
        "\n",
        "  # split up the input and translation\n",
        "  input_text, translation = line.rstrip().split('\\t')\n",
        "\n",
        "  # make the target input and output\n",
        "  # recall we'll be using teacher forcing\n",
        "  target_text = translation + ' <eos>'\n",
        "  target_text_input = '<sos> ' + translation\n",
        "\n",
        "  input_texts.append(input_text)\n",
        "  target_texts.append(target_text)\n",
        "  target_texts_inputs.append(target_text_input)\n",
        "print(\"num samples:\", len(input_texts))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num samples: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK7dhj6cp3W0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec0d3352-1c99-41f2-e3f6-9e0bf89f39aa"
      },
      "source": [
        "# tokenize the inputs\n",
        "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer_inputs.fit_on_texts(input_texts)\n",
        "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
        "\n",
        "# get the word to index mapping for input language\n",
        "word2idx_inputs = tokenizer_inputs.word_index\n",
        "print('Found %s unique input tokens.' % len(word2idx_inputs))\n",
        "\n",
        "# determine maximum length input sequence\n",
        "max_len_input = max(len(s) for s in input_sequences)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2139 unique input tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5IieUP5qBZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize the outputs\n",
        "# don't filter out special characters\n",
        "# otherwise <sos> and <eos> won't appear\n",
        "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
        "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) # inefficient, oh well\n",
        "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
        "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7xH-h2nqFTX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1cd51ebc-6abc-4938-c982-b2714a82ff38"
      },
      "source": [
        "# get the word to index mapping for output language\n",
        "word2idx_outputs = tokenizer_outputs.word_index\n",
        "print('Found %s unique output tokens.' % len(word2idx_outputs))\n",
        "\n",
        "# store number of output words for later\n",
        "# remember to add 1 since indexing starts at 1\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "\n",
        "# determine maximum length output sequence\n",
        "max_len_target = max(len(s) for s in target_sequences)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5771 unique output tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I7kCDduqMVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "01ddfe34-8028-42e0-858d-939449082943"
      },
      "source": [
        "# pad the sequences\n",
        "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
        "print(\"encoder_inputs.shape:\", encoder_inputs.shape)\n",
        "print(\"encoder_inputs[0]:\", encoder_inputs[0])\n",
        "\n",
        "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n",
        "print(\"decoder_inputs[0]:\", decoder_inputs[0])\n",
        "print(\"decoder_inputs.shape:\", decoder_inputs.shape)\n",
        "\n",
        "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder_inputs.shape: (10000, 5)\n",
            "encoder_inputs[0]: [ 0  0  0  0 14]\n",
            "decoder_inputs[0]: [ 2 56  4  0  0  0  0  0  0  0  0]\n",
            "decoder_inputs.shape: (10000, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgKX5fsWqVp5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "9cdcea89-8622-4cdc-b864-5d26ef64300e"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-30 12:35:46--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-10-30 12:35:46--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2019-10-30 12:35:46--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.04MB/s    in 6m 26s  \n",
            "\n",
            "2019-10-30 12:42:12 (2.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y9KRdqWsJ7h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c79f1bae-ab95-416f-88a3-52fdf2799556"
      },
      "source": [
        "!unzip glove*.zip"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SA4dKT_sWEg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "09e26501-c4fa-4864-d20f-4efc0c716168"
      },
      "source": [
        "# store all the pre-trained word vectors\n",
        "import io\n",
        "print('Loading word vectors...')\n",
        "word2vec = {}\n",
        "with io.open('glove.6B.50d.txt', encoding='utf8') as f:\n",
        "  # is just a space-separated text file in the format:\n",
        "  # word vec[0] vec[1] vec[2] ...\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vec = np.asarray(values[1:], dtype='float32')\n",
        "    word2vec[word] = vec\n",
        "print('Found %s word vectors.' % len(word2vec))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading word vectors...\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_47FJj5sbO5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de026e42-93f2-4266-b606-4390a6a427eb"
      },
      "source": [
        "# prepare embedding matrix\n",
        "print('Filling pre-trained embeddings...')\n",
        "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
        "embedding_matrix = np.zeros((num_words, 50))\n",
        "for word, i in word2idx_inputs.items():\n",
        "  if i < MAX_NUM_WORDS:\n",
        "    embedding_vector = word2vec.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all zeros.\n",
        "      embedding_matrix[i] = embedding_vector\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filling pre-trained embeddings...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNmR6sWvspvs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8d19e511-e733-4998-b922-9c2ea9b2fde5"
      },
      "source": [
        "# create embedding layer\n",
        "embedding_layer = Embedding(\n",
        "  num_words,\n",
        "  EMBEDDING_DIM,\n",
        "  weights=[embedding_matrix],\n",
        "  input_length=max_len_input,\n",
        "  # trainable=True\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImWO099jsuHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create targets, since we cannot use sparse\n",
        "# categorical cross entropy when we have sequences\n",
        "decoder_targets_one_hot = np.zeros(\n",
        "  (\n",
        "    len(input_texts),\n",
        "    max_len_target,\n",
        "    num_words_output\n",
        "  ),\n",
        "  dtype='float32'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORTsQFrzsxtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# assign the values\n",
        "for i, d in enumerate(decoder_targets):\n",
        "  for t, word in enumerate(d):\n",
        "    decoder_targets_one_hot[i, t, word] = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDoVBVAZs1i2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "de2eb92b-eabc-4440-8215-9f9d9f22d6ce"
      },
      "source": [
        "##### build the model #####\n",
        "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
        "x = embedding_layer(encoder_inputs_placeholder)\n",
        "encoder = LSTM(\n",
        "  LATENT_DIM,\n",
        "  return_state=True,\n",
        "  # dropout=0.5 # dropout not available on gpu\n",
        ")\n",
        "encoder_outputs, h, c = encoder(x)\n",
        "# encoder_outputs, h = encoder(x) #gru\n",
        "\n",
        "# keep only the states to pass into decoder\n",
        "encoder_states = [h, c]\n",
        "# encoder_states = [state_h] # gru\n",
        "\n",
        "# Set up the decoder, using [h, c] as initial state.\n",
        "decoder_inputs_placeholder = Input(shape=(max_len_target,))\n",
        "\n",
        "# this word embedding will not use pre-trained vectors\n",
        "# although you could\n",
        "decoder_embedding = Embedding(num_words_output, LATENT_DIM)\n",
        "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
        "\n",
        "# since the decoder is a \"to-many\" model we want to have\n",
        "# return_sequences=True\n",
        "decoder_lstm = LSTM(\n",
        "  LATENT_DIM,\n",
        "  return_sequences=True,\n",
        "  return_state=True,\n",
        "  # dropout=0.5 # dropout not available on gpu\n",
        ")\n",
        "decoder_outputs, _, _ = decoder_lstm(\n",
        "  decoder_inputs_x,\n",
        "  initial_state=encoder_states\n",
        ")\n",
        "\n",
        "# decoder_outputs, _ = decoder_gru(\n",
        "#   decoder_inputs_x,\n",
        "#   initial_state=encoder_states\n",
        "# )\n",
        "\n",
        "# final dense layer for predictions\n",
        "decoder_dense = Dense(num_words_output, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Create the model object\n",
        "model = Model([encoder_inputs_placeholder, decoder_inputs_placeholder], decoder_outputs)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utrzQlTbtAz6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b32e3729-1d78-446e-db5f-4c5b9baec953"
      },
      "source": [
        "# Compile the model and train it\n",
        "model.compile(\n",
        "  optimizer='rmsprop',\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "r = model.fit(\n",
        "  [encoder_inputs, decoder_inputs], decoder_targets_one_hot,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  epochs=EPOCHS,\n",
        "  validation_split=0.2,\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 2.3906 - acc: 0.6743 - val_loss: 2.2023 - val_acc: 0.6870\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 5s 667us/step - loss: 1.7725 - acc: 0.7277 - val_loss: 2.0372 - val_acc: 0.7109\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 5s 663us/step - loss: 1.5561 - acc: 0.7586 - val_loss: 1.8850 - val_acc: 0.7321\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 5s 662us/step - loss: 1.4033 - acc: 0.7787 - val_loss: 1.7769 - val_acc: 0.7513\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 5s 662us/step - loss: 1.2898 - acc: 0.7946 - val_loss: 1.7241 - val_acc: 0.7621\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 5s 662us/step - loss: 1.1997 - acc: 0.8058 - val_loss: 1.7130 - val_acc: 0.7633\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 5s 662us/step - loss: 1.1238 - acc: 0.8140 - val_loss: 1.6750 - val_acc: 0.7652\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 5s 662us/step - loss: 1.0566 - acc: 0.8220 - val_loss: 1.6672 - val_acc: 0.7656\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 5s 661us/step - loss: 0.9955 - acc: 0.8285 - val_loss: 1.6595 - val_acc: 0.7660\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 5s 659us/step - loss: 0.9376 - acc: 0.8342 - val_loss: 1.6604 - val_acc: 0.7672\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 5s 658us/step - loss: 0.8877 - acc: 0.8407 - val_loss: 1.6522 - val_acc: 0.7688\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 5s 673us/step - loss: 0.8441 - acc: 0.8464 - val_loss: 1.6599 - val_acc: 0.7705\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 5s 669us/step - loss: 0.8017 - acc: 0.8535 - val_loss: 1.6674 - val_acc: 0.7710\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 5s 657us/step - loss: 0.7624 - acc: 0.8591 - val_loss: 1.6761 - val_acc: 0.7702\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 5s 655us/step - loss: 0.7270 - acc: 0.8635 - val_loss: 1.6765 - val_acc: 0.7714\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 5s 661us/step - loss: 0.6927 - acc: 0.8691 - val_loss: 1.6671 - val_acc: 0.7732\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 5s 674us/step - loss: 0.6611 - acc: 0.8743 - val_loss: 1.6750 - val_acc: 0.7751\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 5s 656us/step - loss: 0.6331 - acc: 0.8784 - val_loss: 1.6991 - val_acc: 0.7745\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 5s 656us/step - loss: 0.6064 - acc: 0.8831 - val_loss: 1.6926 - val_acc: 0.7750\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 5s 658us/step - loss: 0.5821 - acc: 0.8876 - val_loss: 1.7182 - val_acc: 0.7761\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 5s 659us/step - loss: 0.5587 - acc: 0.8913 - val_loss: 1.7339 - val_acc: 0.7736\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 5s 660us/step - loss: 0.5351 - acc: 0.8950 - val_loss: 1.7267 - val_acc: 0.7760\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 5s 668us/step - loss: 0.5145 - acc: 0.8983 - val_loss: 1.7400 - val_acc: 0.7754\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 5s 669us/step - loss: 0.4937 - acc: 0.9014 - val_loss: 1.7607 - val_acc: 0.7739\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 5s 661us/step - loss: 0.4758 - acc: 0.9043 - val_loss: 1.7724 - val_acc: 0.7734\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 5s 658us/step - loss: 0.4586 - acc: 0.9065 - val_loss: 1.7894 - val_acc: 0.7737\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 5s 666us/step - loss: 0.4427 - acc: 0.9090 - val_loss: 1.7970 - val_acc: 0.7732\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 5s 678us/step - loss: 0.4270 - acc: 0.9115 - val_loss: 1.8084 - val_acc: 0.7735\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 5s 664us/step - loss: 0.4112 - acc: 0.9140 - val_loss: 1.8210 - val_acc: 0.7756\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 5s 665us/step - loss: 0.3981 - acc: 0.9158 - val_loss: 1.8343 - val_acc: 0.7747\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 5s 664us/step - loss: 0.3853 - acc: 0.9174 - val_loss: 1.8397 - val_acc: 0.7744\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 5s 674us/step - loss: 0.3731 - acc: 0.9196 - val_loss: 1.8396 - val_acc: 0.7739\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 5s 678us/step - loss: 0.3617 - acc: 0.9207 - val_loss: 1.8376 - val_acc: 0.7756\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 5s 658us/step - loss: 0.3508 - acc: 0.9221 - val_loss: 1.8557 - val_acc: 0.7750\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 5s 658us/step - loss: 0.3394 - acc: 0.9240 - val_loss: 1.8741 - val_acc: 0.7746\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 5s 665us/step - loss: 0.3307 - acc: 0.9259 - val_loss: 1.8761 - val_acc: 0.7728\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 5s 671us/step - loss: 0.3227 - acc: 0.9261 - val_loss: 1.8833 - val_acc: 0.7747\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 5s 681us/step - loss: 0.3146 - acc: 0.9271 - val_loss: 1.8931 - val_acc: 0.7743\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 5s 676us/step - loss: 0.3074 - acc: 0.9285 - val_loss: 1.8909 - val_acc: 0.7730\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 5s 666us/step - loss: 0.2998 - acc: 0.9297 - val_loss: 1.8986 - val_acc: 0.7726\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 5s 669us/step - loss: 0.2920 - acc: 0.9304 - val_loss: 1.9138 - val_acc: 0.7724\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 5s 668us/step - loss: 0.2844 - acc: 0.9317 - val_loss: 1.9000 - val_acc: 0.7738\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 5s 657us/step - loss: 0.2785 - acc: 0.9327 - val_loss: 1.9187 - val_acc: 0.7714\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 5s 657us/step - loss: 0.2727 - acc: 0.9343 - val_loss: 1.9232 - val_acc: 0.7706\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 5s 656us/step - loss: 0.2680 - acc: 0.9342 - val_loss: 1.9304 - val_acc: 0.7699\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 5s 672us/step - loss: 0.2623 - acc: 0.9357 - val_loss: 1.9239 - val_acc: 0.7714\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 5s 659us/step - loss: 0.2577 - acc: 0.9354 - val_loss: 1.9480 - val_acc: 0.7710\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 5s 666us/step - loss: 0.2525 - acc: 0.9366 - val_loss: 1.9329 - val_acc: 0.7705\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 5s 674us/step - loss: 0.2478 - acc: 0.9379 - val_loss: 1.9414 - val_acc: 0.7690\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 5s 675us/step - loss: 0.2446 - acc: 0.9381 - val_loss: 1.9454 - val_acc: 0.7707\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 5s 680us/step - loss: 0.2407 - acc: 0.9382 - val_loss: 1.9565 - val_acc: 0.7696\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 5s 664us/step - loss: 0.2370 - acc: 0.9394 - val_loss: 1.9532 - val_acc: 0.7699\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 5s 658us/step - loss: 0.2325 - acc: 0.9396 - val_loss: 1.9610 - val_acc: 0.7700\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 5s 657us/step - loss: 0.2274 - acc: 0.9407 - val_loss: 1.9718 - val_acc: 0.7690\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 5s 670us/step - loss: 0.2238 - acc: 0.9406 - val_loss: 1.9816 - val_acc: 0.7690\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 5s 660us/step - loss: 0.2200 - acc: 0.9404 - val_loss: 1.9849 - val_acc: 0.7687\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 5s 662us/step - loss: 0.2156 - acc: 0.9414 - val_loss: 1.9854 - val_acc: 0.7701\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 5s 654us/step - loss: 0.2119 - acc: 0.9419 - val_loss: 1.9976 - val_acc: 0.7682\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 5s 658us/step - loss: 0.2084 - acc: 0.9428 - val_loss: 1.9789 - val_acc: 0.7690\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 5s 677us/step - loss: 0.2054 - acc: 0.9427 - val_loss: 2.0004 - val_acc: 0.7692\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 5s 657us/step - loss: 0.2017 - acc: 0.9430 - val_loss: 2.0003 - val_acc: 0.7678\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 5s 656us/step - loss: 0.1988 - acc: 0.9434 - val_loss: 2.0098 - val_acc: 0.7689\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 5s 662us/step - loss: 0.1973 - acc: 0.9436 - val_loss: 2.0137 - val_acc: 0.7696\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 5s 665us/step - loss: 0.1944 - acc: 0.9444 - val_loss: 2.0139 - val_acc: 0.7684\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 5s 674us/step - loss: 0.1918 - acc: 0.9448 - val_loss: 2.0232 - val_acc: 0.7680\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 5s 663us/step - loss: 0.1905 - acc: 0.9443 - val_loss: 2.0154 - val_acc: 0.7687\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 5s 659us/step - loss: 0.1889 - acc: 0.9450 - val_loss: 2.0303 - val_acc: 0.7683\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 5s 671us/step - loss: 0.1869 - acc: 0.9450 - val_loss: 2.0247 - val_acc: 0.7675\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 5s 673us/step - loss: 0.1857 - acc: 0.9445 - val_loss: 2.0387 - val_acc: 0.7684\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 5s 662us/step - loss: 0.1847 - acc: 0.9460 - val_loss: 2.0368 - val_acc: 0.7685\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 5s 662us/step - loss: 0.1834 - acc: 0.9449 - val_loss: 2.0476 - val_acc: 0.7678\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 5s 662us/step - loss: 0.1821 - acc: 0.9457 - val_loss: 2.0397 - val_acc: 0.7675\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 5s 669us/step - loss: 0.1815 - acc: 0.9449 - val_loss: 2.0487 - val_acc: 0.7687\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 5s 683us/step - loss: 0.1803 - acc: 0.9451 - val_loss: 2.0521 - val_acc: 0.7683\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 5s 666us/step - loss: 0.1795 - acc: 0.9455 - val_loss: 2.0523 - val_acc: 0.7670\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 5s 663us/step - loss: 0.1783 - acc: 0.9458 - val_loss: 2.0569 - val_acc: 0.7684\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 5s 662us/step - loss: 0.1773 - acc: 0.9460 - val_loss: 2.0664 - val_acc: 0.7673\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 5s 681us/step - loss: 0.1766 - acc: 0.9458 - val_loss: 2.0649 - val_acc: 0.7667\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 5s 680us/step - loss: 0.1754 - acc: 0.9449 - val_loss: 2.0778 - val_acc: 0.7667\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 5s 668us/step - loss: 0.1745 - acc: 0.9456 - val_loss: 2.0818 - val_acc: 0.7660\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 5s 657us/step - loss: 0.1732 - acc: 0.9462 - val_loss: 2.0769 - val_acc: 0.7667\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 5s 658us/step - loss: 0.1723 - acc: 0.9461 - val_loss: 2.0841 - val_acc: 0.7658\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 5s 660us/step - loss: 0.1717 - acc: 0.9464 - val_loss: 2.0835 - val_acc: 0.7650\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 5s 656us/step - loss: 0.1708 - acc: 0.9464 - val_loss: 2.0767 - val_acc: 0.7662\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 5s 675us/step - loss: 0.1704 - acc: 0.9464 - val_loss: 2.0853 - val_acc: 0.7657\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 5s 658us/step - loss: 0.1697 - acc: 0.9462 - val_loss: 2.0918 - val_acc: 0.7662\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 5s 656us/step - loss: 0.1692 - acc: 0.9460 - val_loss: 2.0861 - val_acc: 0.7666\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 5s 657us/step - loss: 0.1685 - acc: 0.9463 - val_loss: 2.0909 - val_acc: 0.7665\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 5s 668us/step - loss: 0.1677 - acc: 0.9461 - val_loss: 2.0836 - val_acc: 0.7660\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 5s 652us/step - loss: 0.1667 - acc: 0.9466 - val_loss: 2.0931 - val_acc: 0.7658\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 5s 656us/step - loss: 0.1666 - acc: 0.9464 - val_loss: 2.0912 - val_acc: 0.7654\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 5s 653us/step - loss: 0.1656 - acc: 0.9468 - val_loss: 2.0918 - val_acc: 0.7652\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 5s 654us/step - loss: 0.1655 - acc: 0.9466 - val_loss: 2.0985 - val_acc: 0.7642\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 5s 658us/step - loss: 0.1649 - acc: 0.9462 - val_loss: 2.0982 - val_acc: 0.7643\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 5s 668us/step - loss: 0.1637 - acc: 0.9466 - val_loss: 2.0888 - val_acc: 0.7665\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 5s 664us/step - loss: 0.1634 - acc: 0.9462 - val_loss: 2.1003 - val_acc: 0.7648\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 5s 658us/step - loss: 0.1632 - acc: 0.9465 - val_loss: 2.1121 - val_acc: 0.7642\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 5s 658us/step - loss: 0.1629 - acc: 0.9467 - val_loss: 2.1032 - val_acc: 0.7660\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 5s 661us/step - loss: 0.1623 - acc: 0.9465 - val_loss: 2.1041 - val_acc: 0.7645\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 5s 662us/step - loss: 0.1618 - acc: 0.9464 - val_loss: 2.1064 - val_acc: 0.7651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oHmWf2WvM9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "46451f16-3c6f-4abc-e21e-817c0ebf7194"
      },
      "source": [
        "# plot some data\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDXSllIoQGuhKKRUhNNCVUipC/H/0Le+pVJxcTAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMOnw0J2vPOz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "55a760e8-0d01-4a25-c75a-1eed5dec5a4e"
      },
      "source": [
        "# accuracies\n",
        "plt.plot(r.history['acc'], label='acc')\n",
        "plt.plot(r.history['val_acc'], label='val_acc')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-BhYlB-vS9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The encoder will be stand-alone\n",
        "# From this we will get our initial decoder hidden state\n",
        "encoder_model = Model(encoder_inputs_placeholder, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
        "decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "# decoder_states_inputs = [decoder_state_input_h] # gru\n",
        "\n",
        "decoder_inputs_single = Input(shape=(1,))\n",
        "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hldzfq2JveDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this time, we want to keep the states too, to be output\n",
        "# by our sampling model\n",
        "decoder_outputs, h, c = decoder_lstm(\n",
        "  decoder_inputs_single_x,\n",
        "  initial_state=decoder_states_inputs\n",
        ")\n",
        "# decoder_outputs, state_h = decoder_lstm(\n",
        "#   decoder_inputs_single_x,\n",
        "#   initial_state=decoder_states_inputs\n",
        "# ) #gru\n",
        "decoder_states = [h, c]\n",
        "# decoder_states = [h] # gru\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je5N_FyWviXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The sampling model\n",
        "# inputs: y(t-1), h(t-1), c(t-1)\n",
        "# outputs: y(t), h(t), c(t)\n",
        "decoder_model = Model(\n",
        "  [decoder_inputs_single] + decoder_states_inputs, \n",
        "  [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# map indexes back into real words\n",
        "# so we can view the results\n",
        "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
        "idx2word_trans = {v:k for k, v in word2idx_outputs.items()}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NkI6PQovltv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "  # Encode the input as state vectors.\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  # Generate empty target sequence of length 1.\n",
        "  target_seq = np.zeros((1, 1))\n",
        "\n",
        "  # Populate the first character of target sequence with the start character.\n",
        "  # NOTE: tokenizer lower-cases all words\n",
        "  target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "\n",
        "  # if we get this we break\n",
        "  eos = word2idx_outputs['<eos>']\n",
        "\n",
        "  # Create the translation\n",
        "  output_sentence = []\n",
        "  for _ in range(max_len_target):\n",
        "    output_tokens, h, c = decoder_model.predict(\n",
        "      [target_seq] + states_value\n",
        "    )\n",
        "    # output_tokens, h = decoder_model.predict(\n",
        "    #     [target_seq] + states_value\n",
        "    # ) # gru\n",
        "\n",
        "    # Get next word\n",
        "    idx = np.argmax(output_tokens[0, 0, :])\n",
        "\n",
        "    # End sentence of EOS\n",
        "    if eos == idx:\n",
        "      break\n",
        "\n",
        "    word = ''\n",
        "    if idx > 0:\n",
        "      word = idx2word_trans[idx]\n",
        "      output_sentence.append(word)\n",
        "\n",
        "    # Update the decoder input\n",
        "    # which is just the word just generated\n",
        "    target_seq[0, 0] = idx\n",
        "\n",
        "    # Update states\n",
        "    states_value = [h, c]\n",
        "    # states_value = [h] # gru\n",
        "\n",
        "  return ' '.join(output_sentence)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3GzNyfsv9l_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "b8accf70-293c-49c0-b69f-9d85a63cf93e"
      },
      "source": [
        "while True:\n",
        "  # Do some test translations\n",
        "  i = np.random.choice(len(input_texts))\n",
        "  input_seq = encoder_inputs[i:i+1]\n",
        "  translation = decode_sequence(input_seq)\n",
        "  print('-')\n",
        "  print('Input:', input_texts[i])\n",
        "  print('Translation:', translation)\n",
        "\n",
        "  ans = input(\"Continue? [Y/n]\")\n",
        "  if ans and ans.lower().startswith('n'):\n",
        "    break\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input: Eat and drink.\n",
            "Translation: mange et bois.\n",
            "Continue? [Y/n]Y\n",
            "-\n",
            "Input: Is it any good?\n",
            "Translation: est-ce mon sentie ?\n",
            "Continue? [Y/n]Y\n",
            "-\n",
            "Input: Are you dressed?\n",
            "Translation: êtes-vous es-tu laissez-moi\n",
            "Continue? [Y/n]Y\n",
            "-\n",
            "Input: Feel this.\n",
            "Translation: sens ça !\n",
            "Continue? [Y/n]Y\n",
            "-\n",
            "Input: Get in the van.\n",
            "Translation: grimpe dans la camionnette !\n",
            "Continue? [Y/n]Y\n",
            "-\n",
            "Input: Any questions?\n",
            "Translation: de quelconques questions ?\n",
            "Continue? [Y/n]n\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}